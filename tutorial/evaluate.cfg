# Prodigy DSPy Tutorial: Evaluation Configuration
#

# This config is used by the dspy.evaluate recipe to run the baseline program
# against gold-standard data and collect human feedback on metric quality.
#
# Usage:
#   prodigy dspy.evaluate summaries_raw_feedback dataset:summaries_gold \
#     ./evaluate.cfg --debug-metric -F ./components.py

# ============================================================================
# DSPy Language Model Configuration
# ============================================================================
[dspy]
[dspy.lm]
model = "openai/gpt-4.1"
max_tokens = 1500
env_api_key = "OPENAI_API_KEY"
temperature = 0.7


# ============================================================================
# Program Configuration
# ============================================================================
[program]
@dspy_programs = "clinical_summarization.v1"

# ============================================================================
# Metric Configuration
# ============================================================================
# Start with the baseline BERTScore metric to see its limitations
[metric]
@dspy_metrics = "baseline_bertscore.v1"

# Alternative: Try the composite metric with NER entity overlap
#[metric]
#@dspy_metrics = "composite_ner_metric.v1"

# Alternative: Use LLM-as-a-judge for better human alignment
#[metric]
#@dspy_metrics = "holistic_llm_metric.v1"


# ============================================================================
# Evaluator Configuration
# ============================================================================
[evaluator]
# The evaluator class defines the evaluation logic and report structure
# Note: program and metric are injected by the recipe via set_program/set_metric
@dspy_evaluators = "clinical_summarization_evaluator.v1"
# Optional: Control parallel evaluation threads
# num_threads = 4


# ============================================================================
# Data Converters
# ============================================================================

# Shared converter for all recipes
[to_dspy_converter]
# Convert gold-standard Prodigy examples to DSPy format
@dspy_converters = "prodigy_to_dspy.clinical_summary.v1"

# Alternatively, a recipe-specific converter can be specified.
# Use this if evaluate needs different data transformation than annotate/optimize
# [evaluate_to_dspy_converter]
# @dspy_converters = "evaluate_to_dspy.clinical_summary.v1"


# ============================================================================
# Evaluation UI Configuration
# ============================================================================
[dspy_evaluation_ui]
# UI for reviewing predictions and collecting structured feedback
@dspy_ui = "clinical_evaluation_ui.v1"
