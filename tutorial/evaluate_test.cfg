# Prodigy DSPy Tutorial: Test Set Evaluation Configuration
#
# This config is used by the dspy.evaluate recipe to run both the baseline
# and optimized program on a held-out test set for final evaluation.
#
# Usage:
#   prodigy dspy.evaluate summaries_test_ assets/clinical_notes_test.jsonl ./evaluate_test.cfg \
#     -L outputs/optimized_summarizer_v1.json -F ./components.py

# ============================================================================
# DSPy Language Model Configuration
# ============================================================================
[dspy]
[dspy.lm]
model = "openai/gpt-4.1"
max_tokens = 1500
env_api_key = "OPENAI_API_KEY"
temperature = 0.7


# ============================================================================
# Program Configuration
# ============================================================================
[program]
@dspy_programs = "clinical_summarization.v1"


# ============================================================================
# Metric Configuration
# ============================================================================
# Use the LLM-as-a-judge metric for consistent evaluation
[metric]
@dspy_metrics = "holistic_llm_metric.v1"


# ============================================================================
# Evaluator Configuration
# ============================================================================
[evaluator]
# Note: program and metric are injected by the recipe via set_program/set_metric
@dspy_evaluators = "clinical_summarization_evaluator.v1"
num_threads = 1


# ============================================================================
# Data Converters
# ============================================================================
[to_dspy_converter]
@dspy_converters = "prodigy_to_dspy.clinical_summary.v1"


# ============================================================================
# Evaluation UI Configuration
# ============================================================================
[dspy_evaluation_ui]
@dspy_ui = "clinical_evaluation_ui.v1"
